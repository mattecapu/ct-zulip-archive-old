[
    {
        "content": "<p>I've been working through a lot of the stuff I know about probability and inference in a Markov category framework, and seem to be making good progress and generating nice insights.</p>\n<p>However, sometimes I need to do things that involve improper priors, meaning measures that can't be normalised, but can become normalisable after applying Bayes' theorem. I'm wondering if there is any work on making improper priors meaningful in a Markov category or probability monad framework. </p>\n<p>I realise I could just work in a category of unnormalised measures. But that seems like introducing a lot of heavy machinery that might be unnecessary - intuitively, it seems like it should be possible to define a Markov category (in which delete is still natural), where morphisms can represent improper distributions as well as proper ones. Has anything like this been done? (<span class=\"user-mention\" data-user-id=\"276702\">@Tobias Fritz</span>, do you know?)</p>",
        "id": 210671223,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1600615150
    },
    {
        "content": "<p>That's a great question! I haven't haven't worked with improper priors before, but <span class=\"user-mention\" data-user-id=\"308397\">@Sam Staton</span> has, so hopefully he can tell us more <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span> One thing that I imagine he will say is that working with unnormalized measures doesn't require a lot of additional heavy machinery. In the synthetic approach of Markov categories, you can just drop the naturalness of delete, and most things can still be done with the proper additional care; but I'm sure that you know this already.</p>",
        "id": 210680170,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1600628619
    },
    {
        "content": "<p>Hi! The <a href=\"http://www.cs.ox.ac.uk/people/samuel.staton/papers/esop2017.pdf\">category of s-finite kernels</a> works fine for unnormalized/unnormalizable distributions. On the axiomatic side, I think Tobias is right about dropping the naturalness of delete (CD categories?) although there was that <a href=\"#narrow/stream/238032-Categorical-Probability.20and.20Statistics.202020.20workshop/topic/Categorical.20Radon-Nikodym/near/201583254\">conjecture about disintegration/Radon-Nikodym on the CPS stream</a> that we should finish!</p>",
        "id": 210723125,
        "sender_full_name": "Sam Staton",
        "timestamp": 1600683472
    },
    {
        "content": "<p>Actually I came to this more worried about improper <em>posteriors</em> and unnormalizable sub-expressions than priors. In some sense you can end up with these even with <em>proper</em> priors. I wrote a bit more about this from page 13 of <a href=\"https://www.cs.ox.ac.uk/people/samuel.staton/papers/2020cup-chapter.pdf\">my chapter here</a>.</p>",
        "id": 210723205,
        "sender_full_name": "Sam Staton",
        "timestamp": 1600683514
    },
    {
        "content": "<p>Also, I have been wondering whether this synthetic or categorical perspective can give a better understanding of what improper priors really mean. For example, in the Rado topos towards the end of <a href=\"#narrow/stream/238032-Categorical-Probability.20and.20Statistics.202020.20workshop/topic/Jun.205.20-.20Sam.20Staton's.20talk/near/199936426\">my CPS talk</a>, there is a “measure” that is a bit like the uniform distribution on the nodes of the Rado graph, and yet it still forms a Markov category.</p>",
        "id": 210723325,
        "sender_full_name": "Sam Staton",
        "timestamp": 1600683616
    },
    {
        "content": "<p>Thank you both. My worry about introducing unnormalised kernels was just that, having got used to working in a Markov category context where delete is natural, I'm unsure how much I'd have to change to make everything still work if it's not. It could be that it's not very much after all, I haven't really thought about it that much. It seems like you'd need an explicit normalising operation though, and ideally a way to reason with it in string diagrams.</p>\n<p>It still seems like it might not be necessary though. For example, I'm wondering if we can define a Markov category where instead of demanding the kernels be normalised, we make the morphisms equivalence classes of kernels (let's say s-finite ones), where two kernels are considered the same if they're the same up to normalisation, sort of like how states in quantum mechanics are defined as rays in Hilbert space instead of normalised vectors. I'm imagining doing that in such a way that \"proper\" kernels would behave the way they normally do, while improper ones would also exist and behave reasonably. I'm not sure how the tensor product should be defined in this case though.</p>\n<p>Do your intuitions say something like that should be possible or impossible? If it works it might be a neat way to get towards Sam's \"what are they really\" question.</p>",
        "id": 210818618,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1600736198
    },
    {
        "content": "<p>Hi, that's a curious idea, but I suspect it wouldn't work well with composition. Here's an example based on importance sampling. Let p and q be probability measures on R with densities f and g respectively, and suppose p has full support. For a function h : R-&gt;R, let weight(h):R-&gt;R be the measure kernel given by weight(h)(r)(U) = h(r) if r in U, 0 otherwise. So it's like dirac, but unnormalized. Then the composite of kernels<br>\n1 --p--&gt; R --weight(1/f)--&gt; R --weight(g)--&gt; R<br>\nis the same as q, hence normalized.<br>\nBut the composite of the first two morphisms is not normalizable –– it is the Lebesgue measure.</p>",
        "id": 210841250,
        "sender_full_name": "Sam Staton",
        "timestamp": 1600763637
    },
    {
        "content": "<p>Is that a problem? It seems like this example is the kind of thing we would like to allow. Or I would like to, anyway, if it can be done without causing contradictions. It's actually quite nice if you can have a kernel from <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">R</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{R}</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68889em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span></span></span></span></span> that represents the Lebesgue measure - the question is just whether we can have that while still being in a Markov category. (i.e. while still having delete be natural)</p>",
        "id": 210842299,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1600764363
    },
    {
        "content": "<p>I guess by \"the same up to normalisation\" what I mean is something along the lines of \"we consider measures <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">g</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span></span></span></span> the same if there exists some <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi><mo>∈</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">∞</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\lambda\\in (0,\\infty)</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73354em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">∞</span><span class=\"mclose\">)</span></span></span></span> such that, for all events <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">U</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span></span></span></span> in the sample space, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>λ</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>U</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(U)=\\lambda g(U)</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">λ</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mclose\">)</span></span></span></span>.\" So two measures can be the same up to normalisation without being normalisable. For kernels we probably need <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">λ</span></span></span></span> to be able to depend on the parameters.</p>",
        "id": 210842856,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1600764805
    },
    {
        "content": "<p>Thanks. My concern is that in my example, weight(1/f) and weight(g) would be equated with dirac, but you wouldn't want to equate p and q.</p>",
        "id": 210843156,
        "sender_full_name": "Sam Staton",
        "timestamp": 1600765044
    },
    {
        "content": "<p>Ah, I got it. Thanks, that's a good point. I guess composition would have to be defined so they behave like identities, not like weightings. Let me think about that.</p>",
        "id": 210843250,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1600765093
    },
    {
        "content": "<p>Here are some (slightly off-topic) comments on the naturality of detete. I'm very much on board with <span class=\"user-mention\" data-user-id=\"276071\">@Nathaniel Virgo</span>'s idea here, assuming that that is possible at all. The naturality of delete is so convenient in practice that I wouldn't want to give up on it unless it's strictly necessary. On the other hand, dropping it <em>does</em> seem to be necessary to formalize some aspects of measure-theoretic probability which are often considered central, such as the treatment of densities and the Radon-Nikodym theorem (see the <a href=\"#narrow/stream/238032-Categorical-Probability.20and.20Statistics.202020.20workshop/topic/Categorical.20Radon-Nikodym/near/201583254\">discussion</a> Sam had linked to). More precisely, I also have some ideas on how to treat densities even with delete being natural by defining a density to be a deterministic morphism to a suitable \"real number object\", but so far it's only a vague idea, and I doubt that such a treatment would be as elegant as the one of taking a density to be simply a morphism to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span aria-hidden=\"true\" class=\"katex-html\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>, which is what happens without naturality of delete.</p>\n<p>So perhaps the situation is a bit like with symmetric vs braided monoidal categories: most things which can be done with SMC's can be done more generally for BMC's, and in some contexts (such as knot theory) one really needs the greater generality of BMC's. But many people still work with SMC's in practice because of the greater convenience. So we end up having two separate but closely related formalisms. My current impression is that something like this is happening with Markov categories and the naturality of delete as well.</p>\n<p>I wanted to mention this also in order to try and get some input from you on what a good naming scheme would be. The term \"Markov category\" seems to have caught on, which I'm very happy about, and (at least currently) the naturality of delete is included in the axioms. With this axiom dropped, I've recently seen the term \"CD category\" being used (going back to Cho and Jacobs). But probably it would be better to uniformize the terminology a bit, no? So what would be a good pair of terms which makes clear that there are two slightly different but closely related concepts? And which one of these two should the plain \"Markov category\" stand for, if any? One possibility could be to say \"Markov category\" for the non-natural-deletion case, and \"affine Markov category\" or \"normalized Markov category\" for the natural-deletion case. Or something the other way around. Any thoughts on this? (We can move this to a separate thread if it's too distracting here.)</p>",
        "id": 210849424,
        "sender_full_name": "Tobias Fritz",
        "timestamp": 1600768800
    },
    {
        "content": "<p>I'd suggest not changing the meaning of \"Markov category\", because the meaning of delete being natural seems fairly established now, and changing it would be confusing.</p>\n<p>But the distinction between normalised and unnormalised kernels reminds me of the distinction between directed graphical models (i.e. Bayesian networks) and undirected graphical models (i.e. Markov random fields), and the physicist in me tends to think of an unnormalised measure as being something like a factor in a Gibbs measure. This makes me want to very tentatively suggest \"Gibbs category\" as the name for a copy-delete category in the context of probability applications.</p>",
        "id": 210851674,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1600770295
    },
    {
        "content": "<p>I also think \"Markov category\" and \"unnormalised Markov category\" work quite well as terms.</p>",
        "id": 210852946,
        "sender_full_name": "Nathaniel Virgo",
        "timestamp": 1600771282
    },
    {
        "content": "<p>I'm fine with unnormalized Markov category too. <br>\nAs you probably know, in the unnormalized setting, there is a related notion which has been called \"synthetic measure theory\". This is a ccc with countable sums and products and with a commutative monad M such that M takes sums to products.</p>",
        "id": 210853375,
        "sender_full_name": "Sam Staton",
        "timestamp": 1600771527
    }
]