[
    {
        "content": "<p><a href=\"https://arxiv.org/abs/2301.01445\">https://arxiv.org/abs/2301.01445</a>   This is absolutely amazing and I think the authors are here on this Zulip <span class=\"user-mention\" data-user-id=\"275965\">@Evan Patterson</span>  <span class=\"user-mention\" data-user-id=\"275927\">@James Fairbanks</span>  I just want to share as I'm reading this.</p>",
        "id": 319670160,
        "sender_full_name": "Sichu Lu",
        "timestamp": 1672951745
    },
    {
        "content": "<p>Thank you for the kind words!</p>",
        "id": 319670863,
        "sender_full_name": "Evan Patterson",
        "timestamp": 1672952068
    },
    {
        "content": "<p>I'm still digesting this paper so apologies if I am asking stupid things but do you see a possible synthesis of your work describing the mathematical observables with the work done by deepmind and others using large language models to empirically train useful models? see here <a href=\"https://www.nature.com/articles/s41592-021-01252-x\">https://www.nature.com/articles/s41592-021-01252-x</a></p>",
        "id": 319671724,
        "sender_full_name": "Sichu Lu",
        "timestamp": 1672952438
    },
    {
        "content": "<p>I'm not familiar with that line of work, so I can't say much about it specifically. Here's a vague thought instead.</p>\n<p>Our paper is about the conventional way to build scientific models: by using a lot of domain knowledge to handcraft a model with not-too-many parameters and then fitting it to data. When this works, it works really well because the model is latching onto reliable mechanisms and regularities in nature. But doing this requires a lot of labor and expertise, and it doesn't scale very well with traditional methods. (Part of what we're trying to do is make it more scalable by formalizing the compositional aspects of model building, but that's a different story.) Nowadays, people, like the authors of the paper you cite, are trying to use deep neural nets and other black-box prediction algorithms to replace all that bothersome labor with big piles of data. They've had some impressive successes, but these models also suffer from various problems, of which the most significant is, I think, the frequent failure of <a href=\"https://en.wikipedia.org/wiki/External_validity\">external validity</a>. Ben Recht has a nice <a href=\"http://www.argmin.net/2022/03/15/external-validity/\">blog post</a> about that.</p>\n<p>It's natural to wonder whether and how these two approaches to modeling could be reconciled or integrated. That seems like the sort of thing that category theory could help with. But don't ask me how, because I don't know yet :)</p>",
        "id": 319709900,
        "sender_full_name": "Evan Patterson",
        "timestamp": 1672972727
    },
    {
        "content": "<p>If you can't speculate on the internet then where can you? :D  I guess where I am coming from is the classic nlab POV that these categories are the \"right\" descriptions/representations. and if you have the right abstractions, then you can use them to simplify the calculations and provide a constraint on the spaces. I notice you cited Uri Alon in your paper and I had very similar ideas while I was working through his youtube lecture/book which is why I was so excited to read your paper and immediately groked what you were trying to say here.  This is just the motivating philosophy and of course I have to implement this onto an algorithm level. As to your point about the large language models not being very good at figuring what's ground truth if asked to do out of distribution tasks, I have been thinking going beyond what is being done with say basic reinforcement learning, like what deepmind has done with alphacode for example by just having a trained datasets that serves as the \"ground truth\".  The idea is that you should be able to get large language models to generate their own hypothesis,(which is basically what they are doing anyway in these models) A friend and I have coined a term for this, we call this unreal computing because in some ways the model already has an internal world model, you just have to get it output it,  using the right prompt engineering in some cases! Basically you can do hypothesis testing by getting it to output it's guesses about the world and then testing this against empirical reality. There is already a community of people doing this based on that deepmind paper I send. Here: <a href=\"https://openbioml.org/\">https://openbioml.org/</a>  I have been in their discord/meetings listening to them.  I think hypothesis testing can be implemented(the details need to be fleshed out but there is this recent paper by Schmidhuber using actor-critic reinforcement learning: <a href=\"https://arxiv.org/abs/2212.14374\">https://arxiv.org/abs/2212.14374</a>) using reinforcement learning! Your work/paper here offers a way to maybe guide the reinforcement learning systems(or maybe this is another unsupervised system with expert rules??, the distinction blurs) towards the right rules and also serves a check on the mathematical modeling of real world objects using the most abstract nonsense there is.</p>",
        "id": 319718262,
        "sender_full_name": "Sichu Lu",
        "timestamp": 1672980499
    },
    {
        "content": "<p>Thanks for the pointer to OpenBioML, seems interesting.</p>",
        "id": 320116636,
        "sender_full_name": "Evan Patterson",
        "timestamp": 1673210014
    }
]