[
    {
        "content": "<p>I'm not sure I even know what \"learning theory\" is!</p>",
        "id": 241649328,
        "sender_full_name": "John Baez",
        "timestamp": 1622916953
    },
    {
        "content": "<p><a href=\"https://golem.ph.utexas.edu/category/2007/09/category_theory_in_machine_lea.html\">https://golem.ph.utexas.edu/category/2007/09/category_theory_in_machine_lea.html</a></p>",
        "id": 241649571,
        "sender_full_name": "ebigram",
        "timestamp": 1622917216
    },
    {
        "content": "<p><a href=\"https://en.wikipedia.org/wiki/Computational_learning_theory\">https://en.wikipedia.org/wiki/Computational_learning_theory</a></p>",
        "id": 241649609,
        "sender_full_name": "ebigram",
        "timestamp": 1622917278
    },
    {
        "content": "<p>Oh, so you're talking about category theory in machine learning?   Some interesting things have happened since 2007.</p>",
        "id": 241649610,
        "sender_full_name": "John Baez",
        "timestamp": 1622917283
    },
    {
        "content": "<p>One is this paper:</p>\n<ul>\n<li>Brendan Fong, David Spivak and  Rémy Tuyéras, <a href=\"https://arxiv.org/abs/1711.10455\">Backprop as functor: a compositional framework for supervised learning</a>.</li>\n</ul>\n<p>It's led to an interesting line of work on \"learners and lenses\".</p>",
        "id": 241649708,
        "sender_full_name": "John Baez",
        "timestamp": 1622917412
    },
    {
        "content": "<p>yup sorry for the vague wording. as a practitioner this is met by my peers with derision if not downright hostility. I was wondering if it is your feeling now that it might actually be useful</p>",
        "id": 241649711,
        "sender_full_name": "ebigram",
        "timestamp": 1622917417
    },
    {
        "content": "<ul>\n<li>Brendan Fong and Michael Johnson, <a href=\"https://arxiv.org/abs/1903.03671\">Lenses as learners</a>.</li>\n</ul>",
        "id": 241649776,
        "sender_full_name": "John Baez",
        "timestamp": 1622917479
    },
    {
        "content": "<p>okay with those kind of names :] TY, I will go do my homework now.</p>",
        "id": 241649849,
        "sender_full_name": "ebigram",
        "timestamp": 1622917562
    },
    {
        "content": "<p>Lenses are a way of thinking about databases.  I think that seeing learning and databases as two ends of a continuum will eventually be very useful, but it will take more research.</p>",
        "id": 241649881,
        "sender_full_name": "John Baez",
        "timestamp": 1622917573
    },
    {
        "content": "<p>Something that seems more <em>instantly</em> useful is work on differential categories, the differential lambda calculus, and differentiable programming, which can be used for gradient descent algorithm.    You can see some references <a href=\"https://www.google.com/search?client=firefox-b-1-d&amp;q=differential+categories+lambda-calculus\">here</a>.</p>",
        "id": 241650027,
        "sender_full_name": "John Baez",
        "timestamp": 1622917748
    },
    {
        "content": "<p>Actually all this stuff I just mentioned should become part of a single theory: a compositional theory of learning based on differential categories and ideas like lenses.   Maybe it's already happening: I'm not keeping up with this stuff.</p>",
        "id": 241650231,
        "sender_full_name": "John Baez",
        "timestamp": 1622918001
    },
    {
        "content": "<p>There are people here who know more.</p>",
        "id": 241650236,
        "sender_full_name": "John Baez",
        "timestamp": 1622918013
    },
    {
        "content": "<p>My browser tells me I visited the top result, so maybe I was already on the righteous path. However, I don't really have much context to evaluate the import/validity of such work; so it is nice to get an authoritative voice to cosign at least investigating it. I am building an unapologetic POC functional (w/ cats) ml framework, so maybe there's enough there to inform a prototype.</p>",
        "id": 241650391,
        "sender_full_name": "ebigram",
        "timestamp": 1622918187
    },
    {
        "content": "<p>I think there's a lot of work on differentiable programming that's closer to \"instantly useful\" than the work based on the differential lambda-calculus and differential categories.   However, I think a really solid foundation for differential programming should involve differential categories, just as a really solid foundation of the semantics of ordinary programming languages involves categories.   (I guess one has to understand the latter before making progress on the former!)</p>",
        "id": 241650506,
        "sender_full_name": "John Baez",
        "timestamp": 1622918330
    },
    {
        "content": "<p>I think all the applications of category theory to machine learning that I just described count as \"research in progress\" rather than finished stuff that will persuade skeptics.</p>",
        "id": 241650595,
        "sender_full_name": "John Baez",
        "timestamp": 1622918448
    },
    {
        "content": "<p>yeah well, I'm already fighting the python ML hegemony, so this will likely be a \"boutique\" and/or purely academic product lol</p>",
        "id": 241650858,
        "sender_full_name": "ebigram",
        "timestamp": 1622918880
    },
    {
        "content": "<p>A lot of us here are fighting various hegemonies.  <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 241651738,
        "sender_full_name": "John Baez",
        "timestamp": 1622919974
    },
    {
        "content": "<p><a href=\"/user_uploads/21317/3UIsbAuzHS5Wm5Mq1wTfvkp2/image.png\">image.png</a>  &lt;-- the only one who can say that with a straight face</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/3UIsbAuzHS5Wm5Mq1wTfvkp2/image.png\" title=\"image.png\"><img src=\"/user_uploads/21317/3UIsbAuzHS5Wm5Mq1wTfvkp2/image.png\"></a></div>",
        "id": 241651936,
        "sender_full_name": "ebigram",
        "timestamp": 1622920215
    },
    {
        "content": "<p>This recent paper <a href=\"https://arxiv.org/abs/2103.01931\">https://arxiv.org/abs/2103.01931</a> nicely connects the lens-y approach to machine learning with reverse derivative categories</p>",
        "id": 241651953,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1622920257
    },
    {
        "content": "<p>Do you know, <span class=\"user-mention\" data-user-id=\"275901\">@Jules Hedges</span>, if anyone yet has tried to use all this lens-y derivative-y stuff to do something \"practical\" in the realm of machine learning?</p>",
        "id": 241652197,
        "sender_full_name": "John Baez",
        "timestamp": 1622920671
    },
    {
        "content": "<p>Eventually there should be all sorts of weird new things one can do.</p>",
        "id": 241652240,
        "sender_full_name": "John Baez",
        "timestamp": 1622920692
    },
    {
        "content": "<p>Not that I know of. This is something we've discussed a bit in Glasgow. To me it's still not clear what the real benefit of this stuff is. The best bet so far seems to be to fall back on the old \"common language\" line - that it's useful for allowing humans to cut through and understand the frankly silly amount of literature on machine learning. Of course we're all category theorists here, so we're biased in thinking that expressing things that way makes them easier to understand</p>",
        "id": 241653473,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1622922612
    },
    {
        "content": "<p>I think people should make things like self-improving databases that learn how to more efficiently answer your queries, or other weirder hybrids...</p>",
        "id": 241653560,
        "sender_full_name": "John Baez",
        "timestamp": 1622922761
    },
    {
        "content": "<p>I imagined it should cut down on some boilerplate, but that's what I generally feel about working with categories (I use Scala's Typelevel Cats stack quite extensively, and find it very practical for writing succinct maintainable expressive code)</p>",
        "id": 241653587,
        "sender_full_name": "ebigram",
        "timestamp": 1622922840
    },
    {
        "content": "<p>also I don't know if there are any benefits to some compilers in doing high-level optimizations</p>",
        "id": 241653646,
        "sender_full_name": "ebigram",
        "timestamp": 1622922904
    },
    {
        "content": "<p>At the moment it's very hard to tell what are the actual bottlenecks in machine learning research, at least as an outsider. I'm pretty sure that one of the biggest bottlenecks is understanding the analysis (in the sense of real analysis) side of ML. I'd very much like if our category theoretic ideas helped with doing analysis with very complex architectures, but I haven't seen any evidence that it's possible</p>",
        "id": 241653733,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1622923059
    },
    {
        "content": "<p>the main bottleneck is $$$ to do massive matrix multiplication at a scale of giant FAANG companies IMO lol. But yes I take your point on analaysis which I'm embarassingly weak at</p>",
        "id": 241653796,
        "sender_full_name": "ebigram",
        "timestamp": 1622923177
    },
    {
        "content": "<p>Language design, improving the coding experience etc is much lower hanging fruit, but that also means we're up against people doing the same thing using only experience and common sense, not heavy mathematical tools</p>",
        "id": 241653852,
        "sender_full_name": "Jules Hedges",
        "timestamp": 1622923227
    },
    {
        "content": "<p>I saw a paper today proving that to show a learner will preform well on a task is undecidable</p>",
        "id": 241653919,
        "sender_full_name": "ebigram",
        "timestamp": 1622923346
    },
    {
        "content": "<p>which I guess isn't surprising</p>",
        "id": 241653924,
        "sender_full_name": "ebigram",
        "timestamp": 1622923355
    },
    {
        "content": "<p>but also very much cements to me that ML is largely an empirical enterprise</p>",
        "id": 241653932,
        "sender_full_name": "ebigram",
        "timestamp": 1622923380
    },
    {
        "content": "<p>I think the academic side of it is a bit bloated, and ultimately it's not that deep as it is mostly a matter of scale, but I don't intend  to offend anyone, mostly criticizing my own work</p>",
        "id": 241654078,
        "sender_full_name": "ebigram",
        "timestamp": 1622923569
    },
    {
        "content": "<p>At the risk of throwing us off-topic, I'd like to chime in in support of the idea that the bottleneck in machine learning research is that ML models are a howling cognitive vacuum, and that to make progress new ideas in this direction are needed at the fundamental level.</p>",
        "id": 241902425,
        "sender_full_name": "Chad Nester",
        "timestamp": 1623151423
    },
    {
        "content": "<p>No amount of computational power or improvements in tooling are going to fix this: <a href=\"/user_uploads/21317/c8kJo8fbuhtA3LaetV-xBCxB/ipod.png\">ipod.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/21317/c8kJo8fbuhtA3LaetV-xBCxB/ipod.png\" title=\"ipod.png\"><img src=\"/user_uploads/21317/c8kJo8fbuhtA3LaetV-xBCxB/ipod.png\"></a></div>",
        "id": 241902731,
        "sender_full_name": "Chad Nester",
        "timestamp": 1623151612
    },
    {
        "content": "<p>I was working in machine learning research some 15 years ago... the big realization I had back then was that all these algorithms, all the best algorithms, are just variations on dynamic programming, or what you might call the Hamilton-Jacobi-Bellman-Dijkstra equation. It's all application of the distributivity law: a(b+c)=ab+ac. </p>\n<p>Here is one reference: \"The Generalized Distributive Law\",  Srinivas M. Aji and Robert J. McEliece, 2000. <a href=\"https://www-users.cs.umn.edu/~baner029/Teaching/Fall07/papers/GDL.pdf\">https://www-users.cs.umn.edu/~baner029/Teaching/Fall07/papers/GDL.pdf</a></p>",
        "id": 241906114,
        "sender_full_name": "Simon Burton",
        "timestamp": 1623153224
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/stream/229111-general/topic/Introduce.20Yourself!/near/241649881\">said</a>:</p>\n<blockquote>\n<p>Lenses are a way of thinking about databases.  I think that seeing learning and databases as two ends of a continuum will eventually be very useful, but it will take more research.</p>\n</blockquote>\n<p>Tangent: do lenses have anything to do with graph rewriting? Applying a rule feels a lot like doing a select (LHS) and an update on the view of that selection (RHS).</p>",
        "id": 241914927,
        "sender_full_name": "ww",
        "timestamp": 1623157312
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"229111\" href=\"/#narrow/stream/229111-general/topic/Introduce.20Yourself.21\">#general &gt; Introduce Yourself!</a> by <span class=\"user-mention silent\" data-user-id=\"275932\">Matteo Capucci (he/him)</span></p>",
        "id": 241925013,
        "sender_full_name": "Notification Bot",
        "timestamp": 1623161343
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"276114\">Chad Nester</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/around.20machine.20learning/near/241902731\">said</a>:</p>\n<blockquote>\n<p>No amount of computational power or improvements in tooling are going to fix this: <a href=\"/user_uploads/21317/c8kJo8fbuhtA3LaetV-xBCxB/ipod.png\">ipod.png</a></p>\n</blockquote>\n<p>Now I'm wondering: is the 0.4% classification of the original image as iPod due to some kind of \"semantic leakage\" through apple &gt; Apple &gt; iPod?</p>",
        "id": 241925224,
        "sender_full_name": "Reid Barton",
        "timestamp": 1623161427
    },
    {
        "content": "<p>I guess it could also be that some of the iPods have a little apple logo on them.</p>",
        "id": 241931802,
        "sender_full_name": "Chad Nester",
        "timestamp": 1623163854
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"405160\">ww</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/around.20machine.20learning/near/241914927\">said</a>:</p>\n<blockquote>\n<p>Tangent: do lenses have anything to do with graph rewriting? Applying a rule feels a lot like doing a select (LHS) and an update on the view of that selection (RHS).</p>\n</blockquote>\n<p>I don't know enough about lenses to answer that.  Lenses are very general so one might ask more generally if you can get a lens out of a double pushout rewriting system.  (That's a generalization of graph rewriting.)</p>",
        "id": 241941687,
        "sender_full_name": "John Baez",
        "timestamp": 1623167553
    },
    {
        "content": "<p>In other words: \"I can't answer your question, but I can generalize it.\"  <span aria-label=\"upside down\" class=\"emoji emoji-1f643\" role=\"img\" title=\"upside down\">:upside_down:</span></p>",
        "id": 241941767,
        "sender_full_name": "John Baez",
        "timestamp": 1623167592
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"275920\">John Baez</span> <a href=\"#narrow/stream/229156-practice.3A-applied.20ct/topic/around.20machine.20learning/near/241649328\">said</a>:</p>\n<blockquote>\n<p>I'm not sure I even know what \"learning theory\" is!</p>\n</blockquote>\n<p>I can try to explain the big picture. A core problem for any predictive model fitted to data is to estimate how well it will do on future data. We generically expect it will do worse: the predictive error on the training data will be optimistically biased because the fitting procedure is trying to minimize that error! So the question is how to estimate the gap between the expected future error and empirical error. When people talk about theoretical understanding of predictive models, part of what they mean is having provable control over this gap under some set of assumptions.</p>\n<p>There are several major theoretical approaches to this problem. The classical approach relies on distributional assumptions or asymptotics: you either assume a tractable distribution, most often the Gaussian, or use asympotics such as the CLT to get one, and then make explicit calculations. When it works, this approach is excellent because it produces tight bounds. However, it does not scale easily to complex models and methods, although there is interesting recent work on asympotics in high-dimensional regimes.</p>\n<p>Computational learning theory is centered around newer approaches that are non-asymptotic in that they produce finite-sample bounds on the generalization gap. The key mathematical tool is <a href=\"https://en.wikipedia.org/wiki/Concentration_of_measure\">concentration of measure</a> in high-dimensional space. The resulting bounds are usually too loose to be practically useful (e.g., for producing well-calibrated prediction intervals) but the theory is more general and easily applied to complex models than asympotics. That being said, we seem to be very far from being able to analyze realistic deep learning models using these techniques, hence the often heard complaint that the practice of deep learning has far outpaced the theory.</p>",
        "id": 241980190,
        "sender_full_name": "Evan Patterson",
        "timestamp": 1623183572
    },
    {
        "content": "<p>The work I've been doing since, well, forever, but more systems-matically since the early 90s may fall into this ballpark.</p>\n<p>&bull; <a href=\"https://inquiryintoinquiry.com/2020/12/27/survey-of-inquiry-driven-systems-3/\">Survey of Inquiry Driven Systems</a><br>\n&bull; <a href=\"https://oeis.org/wiki/User:Jon_Awbrey/Prospects_for_Inquiry_Driven_Systems\">Prospects for Inquiry Driven Systems</a><br>\n&bull; <a href=\"https://oeis.org/wiki/Introduction_to_Inquiry_Driven_Systems\">Introduction to Inquiry Driven Systems</a><br>\n&bull; <a href=\"https://oeis.org/wiki/Inquiry_Driven_Systems_%E2%80%A2_Overview\">Inquiry Driven Systems &bull; Inquiry Into Inquiry</a></p>\n<p>Regards,<br>\nJon</p>",
        "id": 242069575,
        "sender_full_name": "Jon Awbrey",
        "timestamp": 1623247217
    }
]